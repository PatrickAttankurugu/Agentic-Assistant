{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85a968da-fa96-4733-8eeb-cb7649c48a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AutoGen.pdf\n",
      "Content: AutoGen: Enabling Next-Gen LLM\n",
      "Applications via Multi-Agent Conversation\n",
      "Qingyun Wu†, Gagan Bansal∗, Jieyu Zhang±, Yiran Wu†, Beibin Li∗\n",
      "Erkang Zhu∗, Li Jiang∗, Xiaoyun Zhang∗, Shaokun Zhang†, Jiale Liu∓\n",
      "Ahmed Awadallah∗, Ryen W. White∗, Doug Burger∗, Chi Wang∗1\n",
      "∗Microsoft Research, †Pennsylvania State University\n",
      "±University of Washington,∓Xidian University\n",
      "Agent Customization\n",
      "Conversable agent\n",
      "Flexible Conversation Patterns\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "…\n",
      "Hierarchical chat\n",
      "Joint chat\n",
      "Multi-Agent Conversations\n",
      "…\n",
      "...\n",
      "\n",
      "Title: Chain-of-thought Prompting.pdf\n",
      "Content: Chain-of-Thought Prompting Elicits Reasoning\n",
      "in Large Language Models\n",
      "Jason Wei\n",
      "Xuezhi Wang\n",
      "Dale Schuurmans\n",
      "Maarten Bosma\n",
      "Brian Ichter\n",
      "Fei Xia\n",
      "Ed H. Chi\n",
      "Quoc V. Le\n",
      "Denny Zhou\n",
      "Google Research, Brain Team\n",
      "{jasonwei,dennyzhou}@google.com\n",
      "Abstract\n",
      "We explore how generating a chain of thought—a series of intermediate reasoning\n",
      "steps—signiﬁcantly improves the ability of large language models to perform\n",
      "complex reasoning. In particular, we show how such reasoning abilities emerge\n",
      "naturally in sufﬁcient...\n",
      "\n",
      "Title: ChatDev.pdf\n",
      "Content: Communicative Agents for Software Development\n",
      "Chen Qian♠\n",
      "Xin Cong♠\n",
      "Wei Liu♠\n",
      "Cheng Yang♣\n",
      "Weize Chen♠\n",
      "Yusheng Su♠\n",
      "Yufan Dang♠\n",
      "Jiahao Li♦\n",
      "Juyuan Xu▲\n",
      "Dahai Li⋆\n",
      "Zhiyuan Liu♠B\n",
      "Maosong Sun♠B\n",
      "♠Tsinghua University\n",
      "♣Beijing University of Posts and Telecommunications\n",
      "♦Dalian University of Technology\n",
      "▲Brown University\n",
      "⋆Modelbest Inc.\n",
      "qianc62@gmail.com\n",
      "liuzy@tsinghua.edu.cn\n",
      "sms@tsinghua.edu.cn\n",
      "Figure 1: ChatDev, our virtual\n",
      ":::\n",
      "chat-powered company for software ::\n",
      "development, brings together\n",
      "\"software agent...\n",
      "\n",
      "Title: Gorrila LLMs.pdf\n",
      "Content: Gorilla: Large Language Model Connected with\n",
      "Massive APIs\n",
      "Shishir G. Patil1∗\n",
      "Tianjun Zhang1,∗\n",
      "Xin Wang2\n",
      "Joseph E. Gonzalez1\n",
      "1UC Berkeley\n",
      "2Microsoft Research\n",
      "sgp@berkeley.edu\n",
      "Abstract\n",
      "Large Language Models (LLMs) have seen an impressive wave of advances re-\n",
      "cently, with models now excelling in a variety of tasks, such as mathematical\n",
      "reasoning and program synthesis. However, their potential to effectively use tools\n",
      "via API calls remains unfulfilled. This is a challenging task even for today’s sta...\n",
      "\n",
      "Title: HuggingGPT.pdf\n",
      "Content: HuggingGPT: Solving AI Tasks with ChatGPT and its\n",
      "Friends in Hugging Face\n",
      "Yongliang Shen1,2,∗, Kaitao Song2,∗,†, Xu Tan2,\n",
      "Dongsheng Li2, Weiming Lu1,†, Yueting Zhuang1,†\n",
      "Zhejiang University1, Microsoft Research Asia2\n",
      "{syl, luwm, yzhuang}@zju.edu.cn, {kaitaosong, xuta, dongsli}@microsoft.com\n",
      "https://github.com/microsoft/JARVIS\n",
      "Abstract\n",
      "Solving complicated AI tasks with different domains and modalities is a key step\n",
      "toward artificial general intelligence. While there are numerous AI models avail-\n",
      "...\n",
      "\n",
      "Title: MM React.pdf\n",
      "Content: MM-REACT\n",
      ": Prompting ChatGPT for Multimodal Reasoning and Action\n",
      "Zhengyuan Yang∗, Linjie Li∗, Jianfeng Wang∗, Kevin Lin∗, Ehsan Azarnasab∗, Faisal Ahmed∗,\n",
      "Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang♠\n",
      "Microsoft Azure AI\n",
      "{zhengyang,lindsey.li,jianfw,keli,ehazar,fiahmed,zliu,ce.liu,nzeng,lijuanw}@microsoft.com\n",
      "Visual Math and Text Reasoning\n",
      "Multi-Image Reasoning\n",
      "Open-World Concept Understanding\n",
      "Video Summarization/Event Localization\n",
      "Multi-Hop Document Understanding\n",
      "Visual-Conditioned Joke/Meme\n",
      "S...\n",
      "\n",
      "Title: Reflecxtion.pdf\n",
      "Content: Reflexion: Language Agents with\n",
      "Verbal Reinforcement Learning\n",
      "Noah Shinn\n",
      "Northeastern University\n",
      "noahshinn024@gmail.com\n",
      "Federico Cassano\n",
      "Northeastern University\n",
      "cassano.f@northeastern.edu\n",
      "Edward Berman\n",
      "Northeastern University\n",
      "berman.ed@northeastern.edu\n",
      "Ashwin Gopinath\n",
      "Massachusetts Institute of Technology\n",
      "agopi@mit.edu\n",
      "Karthik Narasimhan\n",
      "Princeton University\n",
      "karthikn@princeton.edu\n",
      "Shunyu Yao\n",
      "Princeton University\n",
      "shunyuy@princeton.edu\n",
      "Abstract\n",
      "Large language models (LLMs) have been increasingly u...\n",
      "\n",
      "Title: Self-refine.pdf\n",
      "Content: SELF-REFINE:\n",
      "Iterative Refinement with Self-Feedback\n",
      "Aman Madaan1, Niket Tandon2, Prakhar Gupta1, Skyler Hallinan3, Luyu Gao1,\n",
      "Sarah Wiegreffe2, Uri Alon1, Nouha Dziri2, Shrimai Prabhumoye4, Yiming Yang1,\n",
      "Shashank Gupta2, Bodhisattwa Prasad Majumder5, Katherine Hermann6,\n",
      "Sean Welleck2,3, Amir Yazdanbakhsh6, Peter Clark2\n",
      "1Language Technologies Institute, Carnegie Mellon University\n",
      "2Allen Institute for Artificial Intelligence\n",
      "3University of Washington\n",
      "4NVIDIA\n",
      "5UC San Diego 6Google Research, Brain ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def extract_text_from_all_pdfs(directory):\n",
    "    documents = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            text = extract_text_from_pdf(file_path)\n",
    "            documents[filename] = text\n",
    "    return documents\n",
    "\n",
    "# Example usage\n",
    "directory = \"./documents\"\n",
    "documents = extract_text_from_all_pdfs(directory)\n",
    "for title, content in documents.items():\n",
    "    print(f\"Title: {title}\\nContent: {content[:500]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aa4de9-dc38-44b1-b750-81df87dfabbd",
   "metadata": {},
   "source": [
    "The above script extracts text from all PDF files in the documents directory and print the first 500 characters of each document for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2701c384-7ffb-4d59-a13b-857a2979deb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
